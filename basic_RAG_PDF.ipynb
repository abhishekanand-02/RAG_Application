{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7064d673",
   "metadata": {},
   "source": [
    "# RAG application built on gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7faa96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"GOT-OCR-2.0-paper.pdf\")\n",
    "data = loader.load()  # entire PDF is loaded as a single Document\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fd71c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dec2320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94824f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-09-03T06:37:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-09-03T06:37:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'GOT-OCR-2.0-paper.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='OCR feature, is also wasteful.\\nAccordingly, we propose the general OCR theory, i.e., OCR-2.0, to break the bottlenecks of both\\ntraditional and LVLM manners on OCR tasks. We think that a model of OCR 2.0 should have the\\nfollowing essential characteristics:\\n• End-to-end. Compared to OCR-1.0 models with complex procedures, the OCR-2.0 model should\\nenjoy a unified and end-to-end architecture to ensure lower maintenance costs. It is cool that a\\nbeginner can quickly master the entire OCR system in the 2.0 era.\\n• Low training and inference costs. The OCR-2.0 model should not be a chatbot, like LVLM, that\\nfocuses on reasoning tasks. Its focus should be on strong perception and recognition of optical\\ncharacters, so it needs a reasonable number of model parameters in exchange for lower training\\nand inference costs.\\n• Versatility. The OCR-2.0 model’s other important point is versatility, including recognizing more')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b6c760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05636945366859436,\n",
       " 0.004828543867915869,\n",
       " -0.07625909894704819,\n",
       " -0.023642510175704956,\n",
       " 0.053293220698833466]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "#Get an API key: \n",
    "# Head to https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file\n",
    "\n",
    "# Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953ca546",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ceb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is new in GOT-OCR-2.0?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c3e95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b20ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention-guided image captioning models are summarized in\n",
      "Section IV. The analyses of attention and explanations and our\n",
      "proposed LRP-inference ﬁne-tuning strategy are introduced in\n",
      "Section V.\n",
      "Figure 5: The plain text (document) OCR ability of GOT. For double-column documents with high\n",
      "text density, GOT can still handle them well, proving the excellent text perception ability.\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f1e53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7303b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d0444fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e0ba673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry, but the provided context does not contain information about IPHONE_OCR. Therefore, I cannot answer your question.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is new in IPHONE_OCR?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018130f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfbae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
